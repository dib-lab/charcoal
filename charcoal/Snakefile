#
# run with --use-conda for maximal froodiness.
#
# --configfile must be specified on command line.
#
# CTB TODO: bring CLI improvements from grist over (@toplevel, etc.)
import csv, sys, os, json
from snakemake.workflow import srcdir
from charcoal.utils import CSV_DictHelper

strict_val = config.get('strict', '1')
strict_mode = int(strict_val)
if not strict_mode:
    print('** WARNING: strict mode is OFF. Config errors will not force exit.')

force = config.get('force', '0')
force = int(force)
force_param = ''
if force:
    force_param = '--force'

### config stuff loaded from config file
genome_list_file = config['genome_list']
genome_list = [ line.strip() for line in open(genome_list_file, 'rt') ]
genome_list = [ line for line in genome_list if line ]   # remove empty lines

genome_dir = config['genome_dir'].rstrip('/')
output_dir = config['output_dir'].rstrip('/')
stage1_dir = config['output_dir'].rstrip('/') + "/stage1"
stage2_dir = config['output_dir'].rstrip('/') + "/stage2"
report_dir = f'{output_dir}/report'

### verification / strict mode

scaled = config['scaled']
try:
    scaled = int(scaled)
    if scaled < 1 or scaled > 100000:
        raise ValueError
except ValueError:
    print('** ERROR: scaled should be a number between 1 and 100000')
    print('** (it must also match the query database scaled value)')
    if strict_mode:
        sys.exit(-1)

ksize = config['ksize']
try:
    ksize = int(ksize)
    if ksize < 15 or ksize > 101:
        raise ValueError
except ValueError:
    print('** ERROR: ksize should be a number between 15 and 101.')
    print('** (it must also match the query database ksize value)')
    if strict_mode:
        sys.exit(-1)

# verify that all genome files exist -
for filename in genome_list:
    break
    fullpath = os.path.join(genome_dir, filename)
    if not os.path.exists(fullpath):
        print(f'** ERROR: genome file {filename} does not exist in {genome_dir}')
        if strict_mode:
            print('** exiting.')
            sys.exit(-1)

# verify that all query databases exist --
if not config['gather_db']:
    print('** ERROR: must define gather_db')
    if strict_mode:
       sys.exit(-1)

for filename in config['gather_db']:
    if not os.path.exists(filename):
        print(f'** ERROR: database {filename} does not exist.')
        if strict_mode:
            print('** exiting.')
            sys.exit(-1)

# does lineage csv exist?
filename = config['lineages_csv']
if not os.path.exists(filename):
    print(f'** ERROR: lineage CSV {filename} does not exist.')
    if strict_mode:
        print('** exiting.')
        sys.exit(-1)

# read in provided lineages, if any, and verify file.
provided_lineages_file = config.get('provided_lineages', '')
provided_lineages = {}
if provided_lineages_file:
    with open(provided_lineages_file, 'rt') as fp:
        r = csv.reader(fp)
        for row in r:
            genome_filename = row[0]
#            if genome_filename not in genome_list:
#                print(f'** WARNING: lineage was provided for unknown genome {genome_filename}')
#                print(f'** in provided lineages file {provided_lineages_file}')
#                print(f'** ({genome_filename} not in {genome_list_file})')
#                if strict_mode:
#                    sys.exit(-1)
            if not row[1:]:
                print(f'** cannot parse provided lineage for {genome_filename}')
                print(f'** ; is it comma separated?')
                if strict_mode: sys.exit(-1)
            provided_lineages[genome_filename] = row[1:]

    print(f'** read {len(provided_lineages)} provided lineages')

def check_is_fraction(f):
    if f >= 0 and f <= 1:
        return True
    return False

min_f_ident = config['min_f_ident']
min_f_ident = float(min_f_ident)
assert check_is_fraction(min_f_ident), 'min_f_ident should be between 0 and 1'

min_f_major = config['min_f_major']
min_f_major = float(min_f_major)
assert check_is_fraction(min_f_major), 'min_f_major should be between 0 and 1'

min_align_pident = config['min_align_pident']
min_align_pident = int(min_align_pident)
assert min_align_pident > 1.0, 'min_align_pident should be between 1 and 100'
assert min_align_pident <= 100, 'min_align_pident should be between 1 and 100'

min_query_coverage = config['min_query_coverage']
min_query_coverage = float(min_query_coverage)
assert check_is_fraction(min_query_coverage), 'min_query_coverage should be between 0 and 1'

default_match_rank = config['match_rank']

print('** config file checks PASSED!')
print('** from here on out, it\'s all snakemake...')

### utility functions

# mark top-level rules:
_toplevel_rules = []
def toplevel(fn):
    assert fn.__name__.startswith('__')
    _toplevel_rules.append(fn.__name__[2:])
    return fn


def output_files(filename_template, **kw):
    return expand(output_dir + filename_template, **kw)


def get_provided_lineage(w):
    "retrieve a lineage for this filename from provided_lineages dictionary"
    filename = w.f
    if filename in provided_lineages:
        lineage = provided_lineages[filename]
        lineage = ";".join(lineage)
        return lineage
    else:
        return "NA"


def get_hitlist_genomes():
    hit_list_filename = output_dir + '/stage1_hitlist.csv'

    # parse the hitlist file
    hitlist_d = CSV_DictHelper(hit_list_filename, 'genome')

    # get the list of genomes to be filtered.
    hitlist_genomes = [ hitlist_d[hl].genome for hl in hitlist_d \
                          if hitlist_d[hl].filter_at != 'none' ]

    return hitlist_genomes


def get_hitlist_match_accs(g):
    with open(f'{stage2_dir}/{g}.matches.json') as fp:
        matches_info = json.load(fp)

    if matches_info["matches"]:
        match_accs = list(matches_info["matches"])
        return match_accs
    else:
        return []


class Checkpoint_HitListPairs:
    def __init__(self, pattern):
        self.pattern = pattern

    def __call__(self, w):
        global checkpoints

        # wait for the results of 'combine_hit_list'; this will trigger
        # exception until that rule has been run.
        checkpoints.combine_hit_list.get()

        # parse hitlist_genomes,
        hitlist_genomes = get_hitlist_genomes()

        # for each genome, parse the matches file & expand the pattern.
        filenames = []
        for g in hitlist_genomes:
            if not w or w.g == g:         # be robust to no wildcards

                # wait for the results of 'hitlist_make_contigs_matches';
                # this will trigger exception until that rule has been run.
                checkpoints.hitlist_make_contigs_matches_all.get(g=g)

                match_accs = get_hitlist_match_accs(g)
                p = expand(self.pattern, g=g, acc=match_accs)
                filenames.extend(p)

        return filenames

###
### rules!
###

wildcard_constraints:
    size="\d+",
    g='[a-zA-Z0-9._-]+'                   # should be everything but /

# default rule: build report index
@toplevel
rule all:
   input:
       output_dir + '/report/index.html'

# clean rule - clean the contigs
@toplevel
rule clean:
    input:
        expand(output_dir + '/{g}.clean.fa.gz', g=genome_list)

# CTB: alias for 'clean'; remove later after making sure it's not in docs :)
rule all_clean_contigs:
    input:
        expand(output_dir + '/{g}.clean.fa.gz', g=genome_list)

# check config files only
@toplevel
rule check:
    input:
        genome_list_file                  # do nothing - this file should exist

# print out the configuration
@toplevel
rule showconf:
    input:
        genome_list_file
    run:
        import yaml
        print('# full aggregated configuration:')
        print(yaml.dump(config).strip())
        print('# END')

###

###
### stage 1 rules -- use sourmash to make a first round list of candidates.
###

# default rule - produce hit list for stage 1
@toplevel
rule stage1:
    input:
        output_dir + '/stage1_hitlist.csv',
        output_dir + '/stage1_genome_summary.csv',

class Checkpoint_HitListGenomes:
    def __init__(self, pattern):
        self.pattern = pattern

    def __call__(self, w):
        global checkpoints

        # wait for the results of 'combine_hit_list'; this will trigger
        # exception until that rule has been run.
        checkpoints.combine_hit_list.get()

        # now, parse...
        hitlist_genomes = get_hitlist_genomes()

        # & return expanded pattern.
        return expand(self.pattern, g=hitlist_genomes)


def make_moltype_compute_args(moltype):
    if moltype == 'DNA':
        args = "--dna"
    elif moltype in ('protein', 'dayhoff', 'hp'):
        args = "--no-dna --{}".format(moltype)
    return args

# generate a signature for a query genome
# CTB TODO: update for sourmash sketch
rule contigs_sig_wc:
    input:
        genome_dir + '/{g}'
    output:
        stage1_dir + '/{g}.sig'
    conda: 'conf/env-sourmash.yml'
    params:
        scaled = config['scaled'],
        ksize = config['ksize'],
        moltype = make_moltype_compute_args(config['moltype'])
    shell: """
        sourmash compute -k {params.ksize} --scaled {params.scaled} \
            {params.moltype} {input} -o {output}
    """



# run a search, query.x.database.
rule prefetch_all_matches_wc:
    input:
        query = stage1_dir + '/{g}.sig',
        databases = config['gather_db']
    output:
        csv = stage1_dir + '/{g}.matches.csv',
        txt = stage1_dir + '/{g}.matches.txt'
    params:
        moltype = "--{}".format(config['moltype'].lower()),
        gather_scaled = config['gather_scaled'],
        threshold_bp = config['gather_scaled']*3
    conda: 'conf/env-sourmash.yml'
    shell: """
        sourmash prefetch {input.query} {input.databases} -o {output.csv} \
            {params.moltype} --scaled {params.gather_scaled} \
            --threshold-bp {params.threshold_bp} >& {output.txt}
        cat {output.txt}
        touch {output.csv}
    """

@toplevel
rule prefetch_all_matches:
    input:
        expand(stage1_dir + '/{g}.matches.csv', g=genome_list)

# generate contigs taxonomy
rule make_contigs_search_taxonomy_wc:
    input:
        genome = genome_dir + '/{g}',
        genome_sig = stage1_dir + '/{g}.sig',
        matches_csv = stage1_dir + '/{g}.matches.csv',
        lineages = config['lineages_csv'],
        databases = config['gather_db']
    output:
        json = stage1_dir + '/{g}.contigs-tax.json',
    conda: 'conf/env-sourmash.yml'
    params:
        match_rank = default_match_rank,
    shell: """
        python -m charcoal.contigs_search_taxonomy \
            --genome {input.genome} --lineages-csv {input.lineages} \
            --genome-sig {input.genome_sig} \
            --matches-csv {input.matches_csv} \
            --databases {input.databases} \
            --json-out {output.json} \
            --match-rank {params.match_rank}
    """

@toplevel
rule make_contigs_search_taxonomy:
    input:
        expand(stage1_dir + '/{g}.contigs-tax.json', g=genome_list)

# compare taxonomy for contigs in a genome; this generates hit list and
# genome summary outputs.
rule compare_taxonomy_single_wc:
    input:
        json = stage1_dir + '/{g}.contigs-tax.json',
        matches_csv = stage1_dir + '/{g}.matches.csv',
        lineages = config['lineages_csv'],
        provided_lineages = provided_lineages_file,
        databases = config['gather_db'],
        genome_list = genome_list_file,
    output:
        hit_list_csv = stage1_dir + '/{g}.hitlist_for_filtering.csv',
        summary_csv = stage1_dir + '/{g}.genome_summary.csv',
        contam_json = stage1_dir + '/{g}.contam_summary.json',
    conda: 'conf/env-sourmash.yml'
    params:
        input_dir = stage1_dir,
        min_f_major = min_f_major,
        min_f_ident = min_f_ident,
        match_rank = default_match_rank,
    shell: """
        python -m charcoal.compare_taxonomy \
            --input-directory {params.input_dir} \
            --lineages-csv {input.lineages} \
            --provided-lineages {input.provided_lineages} \
            --hit-list {output.hit_list_csv} \
            --genome-summary {output.summary_csv} \
            --contam-summary-json {output.contam_json} \
            --min_f_ident={params.min_f_ident} \
            --min_f_major={params.min_f_major} \
            --match-rank={params.match_rank} \
            {wildcards.g} \
            --databases {input.databases}
    """

# combine all of the individual hit lists into a single hitlist summary file.
checkpoint combine_hit_list:
    input:
        expand(stage1_dir + '/{g}.hitlist_for_filtering.csv', g=genome_list),
    output:
        output_dir + '/stage1_hitlist.csv'
    params:
        sort_by = f"{default_match_rank}_bad_bp"
    shell: """
        python -m charcoal.combine_csvs --sort-by {params.sort_by} --reverse \
            {input} > {output}
    """

# combine all of the individual genome summaries into one file.
rule combine_genome_summary:
    input:
        expand(stage1_dir + '/{g}.genome_summary.csv', g=genome_list),
    output:
        output_dir + '/stage1_genome_summary.csv'
    shell: """
        python -m charcoal.combine_csvs --sort-by genome {input} > {output}
    """

###
### stage 2 rules -- do genome alignments between queries and genbank matches.
###

# stage 2: produce matches detail & genome alignments
@toplevel
rule stage2:
    input:
        output_dir + '/stage2_summary.csv',

# download genbank genome details; make an info.csv file for entry.
rule make_genbank_info_csv:
    output:
        csvfile = 'genbank_info/{acc}.info.csv'
    conda: 'conf/env-genbank.yml'
    shell: """
        python -m charcoal.genbank_genomes {wildcards.acc} \
            --output {output.csvfile}
    """

# download actual genomes!
rule download_matching_genomes_one_by_one:
     input:
         csvfile = 'genbank_info/{acc}.info.csv'
     output:
         genome = "genbank_genomes/{acc}_genomic.fna.gz"
     run:
         with open(input.csvfile, 'rt') as infp:
             r = csv.DictReader(infp)
             rows = list(r)
             assert len(rows) == 1
             row = rows[0]
             acc = row['acc']
             assert wildcards.acc.startswith(acc)
             url = row['genome_url']
             name = row['ncbi_tax_name']

             print(f"downloading genome for acc {acc}/{name} from NCBI...")
             with open(output.genome, 'wb') as outfp:
                 with urllib.request.urlopen(url) as response:
                     content = response.read()
                     outfp.write(content)
                     print(f"...wrote {len(content)} bytes to {output.genome}")

# combine genbank genome details for the matches from stage 1.
rule make_hitlist_matches_info_csv:
    input:
        Checkpoint_HitListPairs('genbank_info/{acc}.info.csv')
    output:
        stage2_dir + '/hitlist-accessions.info.csv',
    shell: """
        python -m charcoal.combine_csvs {input} > {output}
    """

# generates list of contaminant & non-contaminant accessions for genomes
# on the hitlist.
checkpoint hitlist_make_contigs_matches_all:
    input:
        genome = genome_dir + '/{g}',
        genome_sig = stage1_dir + '/{g}.sig',
        matches_csv = stage1_dir + '/{g}.matches.csv',
        databases = config['gather_db'],
        lineages = config['lineages_csv'],
        hitlist = output_dir + '/stage1_hitlist.csv'
    output:
        matches_json = stage2_dir + '/{g}.matches.json',
    conda: 'conf/env-sourmash.yml'
    params:
        match_rank = default_match_rank,
    shell: """
        python -m charcoal.contigs_list_contaminants \
            --genome {input.genome} --lineages-csv {input.lineages} \
            --genome-sig {input.genome_sig} \
            --matches-csv {input.matches_csv} \
            --databases {input.databases} \
            --hitlist {input.hitlist} \
            --json-out {output.matches_json} \
            --match-rank {params.match_rank}
    """

@toplevel
rule hitlist_make_contigs_matches:
    input:
        expand(stage2_dir + '/{g}.matches.json', g=genome_list)

# run a mashmap comparison of two genomes.
rule mashmap_compare:
    input:
        query = genome_dir + '/{g}',
        target = ancient('genbank_genomes/{acc}_genomic.fna.gz'),
    output:
        cmpfile = stage2_dir + '/{g}.x.{acc}.mashmap.align',
        outfile = stage2_dir + '/{g}.x.{acc}.mashmap.out',
    conda: 'conf/env-mashmap.yml'
    shell: """
        mashmap -q {input.query} -r {input.target} -o {output.cmpfile} \
            --pi 95 > {output.outfile}
    """

# postprocess alignments w/taxonomy and summarize.
rule postprocess_alignments:
    input:
        Checkpoint_HitListPairs(stage2_dir + '/{g}.x.{acc}.mashmap.align'),
        genome = genome_dir + '/{g}',
        hit_list_csv = stage1_dir + '/{g}.hitlist_for_filtering.csv',
        matches = f'{stage2_dir}/{{g}}.matches.json',
        hitlist_acc_csv = stage2_dir + '/hitlist-accessions.info.csv', # @CTB?
    output:
        json_out = stage2_dir + '/{g}.stage2.json',
        summary_csv = stage2_dir + '/{g}.stage2.csv',
        report = stage2_dir + '/{g}.postprocess.txt',
    conda: 'conf/env-sourmash.yml'
    params:
        input_dir = stage2_dir,
        min_query_coverage = min_query_coverage,
        min_align_pident = min_align_pident,
    shell: """
        python -m charcoal.postprocess_alignments \
            --input-directory {params.input_dir} \
            --hit-list {input.hit_list_csv} \
            --matches-json {input.matches} \
            --json-out {output.json_out} \
            --summary-csv {output.summary_csv} \
            --min-query-coverage={params.min_query_coverage} \
            --min-align-pident={params.min_align_pident} \
            {input.genome} | tee {output.report}
    """

# combine postprocessing summary details for the matches from stage 2.
rule combine_postprocessing_csv:
    input:
        Checkpoint_HitListGenomes(f'{stage2_dir}/{{g}}.stage2.csv'),
    output:
        output_dir + '/stage2_summary.csv',
    shell: """
        python -m charcoal.combine_csvs --sort-by remove_kb --reverse \
            {input} > {output}
    """

###
### stage 3 rules -- do actual genome cleaning.
###

# actually do cleaning. @CTB out of date, fixme later!
rule clean_contigs:
    input:
        genome = genome_dir + '/{g}',
        json = stage1_dir + '/{g}.contigs-tax.json',
        hit_list = output_dir + '/stage1_hitlist.csv',
    output:
        clean = output_dir + '/{g}.clean.fa.gz',
        dirty = output_dir + '/{g}.dirty.fa.gz',
    conda: 'conf/env-sourmash.yml'
    shell: """
        python -m charcoal.clean_genome \
            --genome {input.genome} \
            --hit-list {input.hit_list} \
            --contigs-json {input.json} \
            --clean {output.clean} --dirty {output.dirty}
    """

###
### reporting rules! run notebooks etc.
###

# report - make HTML output
@toplevel
rule report:
    input:
        expand("{dir}/{g}.fig.html", dir=report_dir, g=genome_list),
        Checkpoint_HitListGenomes(f'{report_dir}/{{g}}.align.html'),
        f'{report_dir}/index.html',
        f'{report_dir}/stage2.html'

# configure notebook kernel; all notebook rules depend on this setup.
rule set_kernel:
    output:
        touch(f"{output_dir}/.kernel.set")
    conda: 'conf/env-reporting.yml'
    shell: """
        python -m ipykernel install --user --name charcoal
        python -m pip install pyinterval
        python -m pip install -e .
    """

# make genome summary ipynb.
rule make_notebook_report:
    input:
        nb='charcoal/notebooks/report-genome.ipynb',
        summary=f'{output_dir}/stage1_genome_summary.csv',
        hitlist=f'{output_dir}/stage1_hitlist.csv',
        kernel_set = rules.set_kernel.output
    output:
        report_dir + '/{g}.fig.ipynb'
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.nb} - -k charcoal --cwd {report_dir} \
              -p directory .. -p render '' \
              -p name {wildcards.g:q} \
              > {output}
    """

# make genome summart HTML.
rule make_html_report:
    input:
        notebook=report_dir + '/{g}.fig.ipynb',
        summary=f'{output_dir}/stage1_genome_summary.csv',
        hitlist=f'{output_dir}/stage1_hitlist.csv',
        contigs_json=f'{output_dir}/stage1/{{g}}.contigs-tax.json',
    output:
        report_dir + '/{g}.fig.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        python -m nbconvert {input.notebook} --to html --stdout --no-input --ExecutePreprocessor.kernel_name=charcoal > {output}
    """
     
# make alignment ipynb.
rule make_notebook_alignment:
    input:
        Checkpoint_HitListPairs(stage2_dir + '/{g}.x.{acc}.mashmap.align'),
        nb = 'charcoal/notebooks/report-alignment.ipynb',
        summary = f'{stage2_dir}/{{g}}.matches.json',
        hitlist_acc_csv = stage2_dir + '/hitlist-accessions.info.csv',
        kernel_set = rules.set_kernel.output
    output:
        report_dir + '/{g}.align.ipynb'
    params:
        rel_genome_dir = os.path.join('../..', genome_dir)
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.nb} - -k charcoal --cwd {report_dir} \
              -p output_dir .. -p genome_dir {params.rel_genome_dir:q} -p render '' \
              -p name {wildcards.g:q} \
              > {output}
    """

# make alignment html.
rule make_html_alignment:
    input:
        notebook=report_dir + '/{g}.align.ipynb',
        summary=f'{stage2_dir}/{{g}}.matches.json',
    output:
        report_dir + '/{g}.align.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        python -m nbconvert {input.notebook} --to html --stdout --no-input --ExecutePreprocessor.kernel_name=charcoal > {output}
    """

# make all the alignment reports.
rule all_alignment_reports:
    input:
        Checkpoint_HitListGenomes(f'{report_dir}/{{g}}.align.html'),

# make the stage 2 index
rule make_stage2_index:
    input:
        Checkpoint_HitListGenomes(f'{report_dir}/{{g}}.align.html'),
        notebook='charcoal/notebooks/report-stage2.ipynb',
        summary=f'{output_dir}/stage2_summary.csv',
        kernel_set = rules.set_kernel.output
    output:
        nb=f'{report_dir}/stage2.ipynb',
        html=f'{report_dir}/stage2.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.notebook} - -p name {output_dir:q} -p render '' \
            -p directory .. -k charcoal --cwd {report_dir} > {output.nb}
        python -m nbconvert {output.nb} --to html --stdout --no-input > {output.html}
    """


# make the index.
rule make_index:
    input:
        Checkpoint_HitListGenomes(f'{report_dir}/{{g}}.fig.html'),
        Checkpoint_HitListGenomes(f'{report_dir}/{{g}}.align.html'),
        notebook='charcoal/notebooks/report-index.ipynb',
        summary=f'{output_dir}/stage1_genome_summary.csv',
        kernel_set = rules.set_kernel.output
    output:
        nb=f'{report_dir}/index.ipynb',
        html=f'{report_dir}/index.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.notebook} - -p name {output_dir:q} -p render '' \
            -p directory .. -k charcoal --cwd {report_dir} > {output.nb}
        python -m nbconvert {output.nb} --to html --stdout --no-input > {output.html}
    """
