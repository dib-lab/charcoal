#
# run with --use-conda for maximal froodiness.
#
# --configfile must be specified on command line.
#
import csv, sys
from snakemake.workflow import srcdir
from charcoal.utils import CSV_DictHelper

strict_val = config.get('strict', '1')
strict_mode = int(strict_val)
if not strict_mode:
    print('** WARNING: strict mode is OFF. Config errors will not force exit.')

force = config.get('force', '0')
force = int(force)
force_param = ''
if force:
    force_param = '--force'

### config stuff loaded from config file
genome_list_file = config['genome_list']
genome_list = [ line.strip() for line in open(genome_list_file, 'rt') ]
genome_list = [ line for line in genome_list if line ]   # remove empty lines

genome_dir = config['genome_dir'].rstrip('/')
output_dir = config['output_dir'].rstrip('/')
report_dir = f'{output_dir}/report'

### verification / strict mode

scaled = config['scaled']
try:
    scaled = int(scaled)
    if scaled < 1 or scaled > 100000:
        raise ValueError
except ValueError:
    print('** ERROR: scaled should be a number between 1 and 100000')
    print('** (it must also match the query database scaled value)')
    if strict_mode:
        sys.exit(-1)

ksize = config['ksize']
try:
    ksize = int(ksize)
    if ksize < 15 or ksize > 101:
        raise ValueError
except ValueError:
    print('** ERROR: ksize should be a number between 15 and 101.')
    print('** (it must also match the query database ksize value)')
    if strict_mode:
        sys.exit(-1)

# verify that all genome files exist -
for filename in genome_list:
    fullpath = os.path.join(genome_dir, filename)
    if not os.path.exists(fullpath):
        print(f'** ERROR: genome file {filename} does not exist in {genome_dir}')
        if strict_mode:
            print('** exiting.')
            sys.exit(-1)

# verify that all query databases exist --
if not config['gather_db']:
    print('** ERROR: must define gather_db')
    if strict_mode:
       sys.exit(-1)

for filename in config['gather_db']:
    if not os.path.exists(filename):
        print(f'** ERROR: database {filename} does not exist.')
        if strict_mode:
            print('** exiting.')
            sys.exit(-1)

# does lineage csv exist?
filename = config['lineages_csv']
if not os.path.exists(filename):
    print(f'** ERROR: lineage CSV {filename} does not exist.')
    if strict_mode:
        print('** exiting.')
        sys.exit(-1)

# read in provided lineages, if any, and verify file.
provided_lineages_file = config.get('provided_lineages', '')
provided_lineages = {}
if provided_lineages_file:
    with open(provided_lineages_file, 'rt') as fp:
        r = csv.reader(fp)
        for row in r:
            genome_filename = row[0]
#            if genome_filename not in genome_list:
#                print(f'** WARNING: lineage was provided for unknown genome {genome_filename}')
#                print(f'** in provided lineages file {provided_lineages_file}')
#                print(f'** ({genome_filename} not in {genome_list_file})')
#                if strict_mode:
#                    sys.exit(-1)
            if not row[1:]:
                print(f'** cannot parse provided lineage for {genome_filename}')
                print(f'** ; is it comma separated?')
                if strict_mode: sys.exit(-1)
            provided_lineages[genome_filename] = row[1:]

    print(f'** read {len(provided_lineages)} provided lineages')

min_f_ident = config['min_f_ident']
min_f_ident = float(min_f_ident)

min_f_major = config['min_f_major']
min_f_major = float(min_f_major)

print('** config file checks PASSED!')
print('** from here on out, it\'s all snakemake...')

### utility functions
def output_files(filename_template, **kw):
    return expand(output_dir + filename_template, **kw)

def get_provided_lineage(w):
    "retrieve a lineage for this filename from provided_lineages dictionary"
    filename = w.f
    if filename in provided_lineages:
        lineage = provided_lineages[filename]
        lineage = ";".join(lineage)
        return lineage
    else:
        return "NA"

###
### rules!
###

wildcard_constraints:
    size="\d+",
    g='[a-zA-Z0-9._-]+'                   # should be everything but /


# default rule - produce hit list
rule inspect:
    input:
        output_dir + '/hit_list_for_filtering.csv'

# clean rule - clean the contigs
rule clean:
    input:
        expand(output_dir + '/{f}.clean.fa.gz', f=genome_list)

# report - make HTML output
rule report:
    input:
        expand("{dir}/{g}.fig.html", dir=report_dir, g=genome_list),
        f'{report_dir}/index.html'

# check config files only
rule check:
    input:
        genome_list_file                  # do nothing - this file should exist

# print out the configuration
rule showconf:
    input:
        genome_list_file
    run:
        import yaml
        print('# full aggregated configuration:')
        print(yaml.dump(config).strip())
        print('# END')

###

def make_moltype_compute_args(moltype):
    if moltype == 'DNA':
        args = "--dna"
    elif moltype in ('protein', 'dayhoff', 'hp'):
        args = "--no-dna --{}".format(moltype)
    return args

# generate a signature
rule contigs_sig:
    input:
        genome_dir + '/{filename}'
    output:
        output_dir + '/{filename}.sig'
    conda: 'conf/env-sourmash.yml'
    params:
        scaled = config['scaled'],
        ksize = config['ksize'],
        moltype = make_moltype_compute_args(config['moltype'])
    shell: """
        sourmash compute -k {params.ksize} --scaled {params.scaled} \
            {params.moltype} {input} -o {output}
    """

# run a search against the database.
# this should be the most time-consuming and memory-intensive step...
rule search_all:
    input:
        query = output_dir + '/{filename}.sig',
        databases = config['gather_db']
    output:
        csv = output_dir + '/{filename}.matches.csv',
        matches = output_dir + '/{filename}.matches.sig',
        txt = output_dir + '/{filename}.matches.txt'
    params:
        moltype = "--{}".format(config['moltype'].lower()),
        gather_scaled = config['gather_scaled']
    conda: 'conf/env-sourmash.yml'
    shell: """
        sourmash search {input.query} {input.databases} -o {output.csv} \
            --containment \
            {params.moltype} --scaled {params.gather_scaled} \
            --save-matches {output.matches} --threshold=0.001 >& {output.txt}
        cat {output.txt}
        touch {output.csv} {output.matches}
    """

# generate contigs taxonomy
rule make_contigs_taxonomy_json:
    input:
        genome = genome_dir + '/{f}',
        genome_sig = output_dir + '/{f}.sig',
        matches = output_dir + '/{f}.matches.sig',
        lineages = config['lineages_csv']
    output:
        json=output_dir + '/{f}.contigs-tax.json',
    conda: 'conf/env-sourmash.yml'
    shell: """
        python -m charcoal.contigs_search \
            --genome {input.genome} --lineages-csv {input.lineages} \
            --genome-sig {input.genome_sig} \
            --matches-sig {input.matches} \
            --json-out {output.json}
    """

# actually do cleaning
#rule contigs_clean_just_taxonomy:
#    input:
##        script = srcdir('just_taxonomy.py'),
#        genome = genome_dir + '/{f}',
#        matches = output_dir + '/{f}.matches.sig',
#        lineages = config['lineages_csv']
#    output:
#        clean=output_dir + '/{f}.clean.fa.gz',
#        dirty=output_dir + '/{f}.dirty.fa.gz',
#        report=output_dir + '/{f}.report.txt',
#        contig_report=output_dir + '/{f}.contigs.csv',
#        csv=output_dir + '/{f}.summary.csv'
#    conda: 'conf/env-sourmash.yml'
#    params:
#        lineage = get_provided_lineage,
#        force = force_param,
#        match_rank = match_rank,
#        moltype = config['moltype']
#    shell: """
#        python -m charcoal.just_taxonomy \
#            --genome {input.genome} --lineages_csv {input.lineages} \
#            --matches_sig {input.matches} \
#            --clean {output.clean} --dirty {output.dirty} \
#            --report {output.report} --summary {output.csv} \
#            --lineage {params.lineage:q} {params.force} \
#            --match-rank {params.match_rank} \
#            --contig-report {output.contig_report}
#    """

# build the combined summary
rule combined_summary:
    input:
        expand(output_dir + '/{g}.summary.csv', g=genome_list),
    output:
        output_dir + '/combined_summary.csv',
    run:
        # combine all of the summary CSV files
        import csv
        with open(output[0], 'wt') as fp:
            w = csv.writer(fp)

            header = ["genomefile", "brieftax",
                      "f_major", "f_ident", "f_removed",
                      "n_reason_1", "n_reason_2", "n_reason_3",
                      "refsize", "ratio",
                      "clean_bp", "clean_n", "dirty_n", "dirty_bp", "missed_n",
                      "missed_bp", "noident_n", "noident_bp",
                      "taxguessed", "taxprovided",
                      "comment"]

            w.writerow(header)

            for i in input:
                with open(i, 'rt') as in_fp:
                   r = csv.reader(in_fp)
                   rows = list(r)
                   assert len(rows) == 1
                   row = rows[0]
                   assert len(row) == len(header), (len(row), len(header), i)

                   w.writerow(row)


# generate a hit list
rule make_hit_list:
    input:
        all_json=expand(output_dir + '/{g}.contigs-tax.json', g=genome_list),
        all_sig=expand(output_dir + '/{g}.matches.sig', g=genome_list),
        lineages = config['lineages_csv'],
        provided_lineages = provided_lineages_file,
        genome_list = genome_list_file
    output:
        hit_list = output_dir + '/hit_list_for_filtering.csv',
        contig_details = output_dir + '/genome_summary.csv',
    conda: 'conf/env-sourmash.yml'
    params:
        output_dir = output_dir,
        min_f_major = min_f_major,
        min_f_ident = min_f_ident,
    shell: """
        python -m charcoal.compare_taxonomy \
            --input-directory {params.output_dir} \
            --genome-list-file {input.genome_list} \
            --lineages-csv {input.lineages} \
            --provided-lineages {input.provided_lineages} \
            --hit-list {output.hit_list} \
            --contig-details-summary {output.contig_details} \
            --min_f_ident={params.min_f_ident} \
            --min_f_major={params.min_f_major}
    """

rule clean_contigs:
    input:
        genome = genome_dir + '/{f}',
        json = output_dir + '/{f}.contigs-tax.json',
        hit_list = output_dir + '/hit_list_for_filtering.csv',
    output:
        clean = output_dir + '/{f}.clean.fa.gz',
        dirty = output_dir + '/{f}.dirty.fa.gz',
    conda: 'conf/env-sourmash.yml'
    shell: """
        python -m charcoal.clean_genome \
            --genome {input.genome} \
            --hit-list {input.hit_list} \
            --contigs-json {input.json} \
            --clean {output.clean} --dirty {output.dirty}
    """

rule make_notebook:
    input:
        'charcoal/notebooks/genome-report.ipynb',
    output:
        report_dir + '/{g}.fig.ipynb'
    params:
        summary_csv = f'{output_dir}/genome_summary.csv',
        name = '{g}',
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input} - -k python3 \
              -p summary_csv {params.summary_csv:q} \
              -p name {params.name:q} \
              > {output}
    """

rule make_html:
    input:
        notebook=report_dir + '/{g}.fig.ipynb',
        summary=f'{output_dir}/genome_summary.csv',
        hitlist=f'{output_dir}/hit_list_for_filtering.csv',
        contigs_json=f'{output_dir}/{{g}}.contigs-tax.json',
    output:
        report_dir + '/{g}.fig.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        python -m nbconvert {input.notebook} --stdout --no-input --ExecutePreprocessor.kernel_name=python3 > {output}
    """
     

rule make_index:
    input:
        notebook='charcoal/notebooks/report_index.ipynb',
        summary=f'{output_dir}/genome_summary.csv',
    output:
        f'{report_dir}/index.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.notebook} - -p directory {output_dir:q} -k python3 | \
            python -m nbconvert --stdin --stdout --no-input > {output}
    """
