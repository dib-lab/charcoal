#
# run with --use-conda for maximal froodiness.
#
# --configfile must be specified on command line.
#
import csv, sys, os
from snakemake.workflow import srcdir
from charcoal.utils import CSV_DictHelper

strict_val = config.get('strict', '1')
strict_mode = int(strict_val)
if not strict_mode:
    print('** WARNING: strict mode is OFF. Config errors will not force exit.')

force = config.get('force', '0')
force = int(force)
force_param = ''
if force:
    force_param = '--force'

### config stuff loaded from config file
genome_list_file = config['genome_list']
genome_list = [ line.strip() for line in open(genome_list_file, 'rt') ]
genome_list = [ line for line in genome_list if line ]   # remove empty lines

genome_dir = config['genome_dir'].rstrip('/')
output_dir = config['output_dir'].rstrip('/')
report_dir = f'{output_dir}/report'

hitlist_path = f'{output_dir}/hit_list_for_filtering.csv'
hitlist_genomes = []
if os.path.exists(hitlist_path):
    from charcoal.utils import CSV_DictHelper
    hitlist_d = CSV_DictHelper(hitlist_path, 'genome')
    hitlist_genomes = [ hitlist_d[hl].genome for hl in hitlist_d \
                          if hitlist_d[hl].filter_at != 'none' ]
    print(f'got {len(hitlist_genomes)} genomes from the hitlist.')

hitlist_pairs = []
for g in hitlist_genomes:
    try:
        g_accs = open(f'{output_dir}/{g}.matches.acc.txt').readlines()
        g_accs = [ g_acc.strip() for g_acc in g_accs ]
        for g_acc in g_accs:
            hitlist_pairs.append((g, g_acc))
    except FileNotFoundError:
        print(f'cannot find accession file for {g}; cancelling hitlist pairs')
        hitlist_pairs = []
        break

print(f'loaded {len(hitlist_pairs)} total genome pairs from hitlist.')

### verification / strict mode

scaled = config['scaled']
try:
    scaled = int(scaled)
    if scaled < 1 or scaled > 100000:
        raise ValueError
except ValueError:
    print('** ERROR: scaled should be a number between 1 and 100000')
    print('** (it must also match the query database scaled value)')
    if strict_mode:
        sys.exit(-1)

ksize = config['ksize']
try:
    ksize = int(ksize)
    if ksize < 15 or ksize > 101:
        raise ValueError
except ValueError:
    print('** ERROR: ksize should be a number between 15 and 101.')
    print('** (it must also match the query database ksize value)')
    if strict_mode:
        sys.exit(-1)

# verify that all genome files exist -
for filename in genome_list:
    break
    fullpath = os.path.join(genome_dir, filename)
    if not os.path.exists(fullpath):
        print(f'** ERROR: genome file {filename} does not exist in {genome_dir}')
        if strict_mode:
            print('** exiting.')
            sys.exit(-1)

# verify that all query databases exist --
if not config['gather_db']:
    print('** ERROR: must define gather_db')
    if strict_mode:
       sys.exit(-1)

for filename in config['gather_db']:
    if not os.path.exists(filename):
        print(f'** ERROR: database {filename} does not exist.')
        if strict_mode:
            print('** exiting.')
            sys.exit(-1)

# does lineage csv exist?
filename = config['lineages_csv']
if not os.path.exists(filename):
    print(f'** ERROR: lineage CSV {filename} does not exist.')
    if strict_mode:
        print('** exiting.')
        sys.exit(-1)

# read in provided lineages, if any, and verify file.
provided_lineages_file = config.get('provided_lineages', '')
provided_lineages = {}
if provided_lineages_file:
    with open(provided_lineages_file, 'rt') as fp:
        r = csv.reader(fp)
        for row in r:
            genome_filename = row[0]
#            if genome_filename not in genome_list:
#                print(f'** WARNING: lineage was provided for unknown genome {genome_filename}')
#                print(f'** in provided lineages file {provided_lineages_file}')
#                print(f'** ({genome_filename} not in {genome_list_file})')
#                if strict_mode:
#                    sys.exit(-1)
            if not row[1:]:
                print(f'** cannot parse provided lineage for {genome_filename}')
                print(f'** ; is it comma separated?')
                if strict_mode: sys.exit(-1)
            provided_lineages[genome_filename] = row[1:]

    print(f'** read {len(provided_lineages)} provided lineages')

min_f_ident = config['min_f_ident']
min_f_ident = float(min_f_ident)

min_f_major = config['min_f_major']
min_f_major = float(min_f_major)

default_match_rank = config['match_rank']

print('** config file checks PASSED!')
print('** from here on out, it\'s all snakemake...')

### utility functions
def output_files(filename_template, **kw):
    return expand(output_dir + filename_template, **kw)

def get_provided_lineage(w):
    "retrieve a lineage for this filename from provided_lineages dictionary"
    filename = w.f
    if filename in provided_lineages:
        lineage = provided_lineages[filename]
        lineage = ";".join(lineage)
        return lineage
    else:
        return "NA"

###
### rules!
###

wildcard_constraints:
    size="\d+",
    g='[a-zA-Z0-9._-]+'                   # should be everything but /


# default rule - produce hit list
rule inspect:
    input:
        output_dir + '/hit_list_for_filtering.csv',
        output_dir + '/genome_summary.csv',
        output_dir + '/hitlist.acc.txt'

# clean rule - clean the contigs
rule clean:
    input:
        expand(output_dir + '/{f}.clean.fa.gz', f=genome_list)

# report - make HTML output
rule report:
    input:
        expand("{dir}/{g}.fig.html", dir=report_dir, g=genome_list),
        expand("{dir}/{g}.align.html", dir=report_dir, g=hitlist_genomes),
        f'{report_dir}/index.html'

# check config files only
rule check:
    input:
        genome_list_file                  # do nothing - this file should exist

# print out the configuration
rule showconf:
    input:
        genome_list_file
    run:
        import yaml
        print('# full aggregated configuration:')
        print(yaml.dump(config).strip())
        print('# END')

###

def make_moltype_compute_args(moltype):
    if moltype == 'DNA':
        args = "--dna"
    elif moltype in ('protein', 'dayhoff', 'hp'):
        args = "--no-dna --{}".format(moltype)
    return args

# generate a signature
rule contigs_sig:
    input:
        genome_dir + '/{filename}'
    output:
        output_dir + '/{filename}.sig'
    conda: 'conf/env-sourmash.yml'
    params:
        scaled = config['scaled'],
        ksize = config['ksize'],
        moltype = make_moltype_compute_args(config['moltype'])
    shell: """
        sourmash compute -k {params.ksize} --scaled {params.scaled} \
            {params.moltype} {input} -o {output}
    """

# run a search against the database.
# this should be the most time-consuming and memory-intensive step...
rule search_all:
    input:
        query = output_dir + '/{filename}.sig',
        databases = config['gather_db']
    output:
        csv = output_dir + '/{filename}.matches.csv',
        matches = output_dir + '/{filename}.matches.sig',
        txt = output_dir + '/{filename}.matches.txt'
    params:
        moltype = "--{}".format(config['moltype'].lower()),
        gather_scaled = config['gather_scaled']
    conda: 'conf/env-sourmash.yml'
    shell: """
        sourmash search {input.query} {input.databases} -o {output.csv} \
            --containment \
            {params.moltype} --scaled {params.gather_scaled} \
            --save-matches {output.matches} --threshold=0.001 >& {output.txt}
        cat {output.txt}
        touch {output.csv} {output.matches}
    """

# generate contigs taxonomy
rule make_contigs_taxonomy_json:
    input:
        genome = genome_dir + '/{f}',
        genome_sig = output_dir + '/{f}.sig',
        matches = output_dir + '/{f}.matches.sig',
        lineages = config['lineages_csv']
    output:
        json=output_dir + '/{f}.contigs-tax.json',
    conda: 'conf/env-sourmash.yml'
    params:
        match_rank = default_match_rank,
    shell: """
        python -m charcoal.contigs_search_taxonomy \
            --genome {input.genome} --lineages-csv {input.lineages} \
            --genome-sig {input.genome_sig} \
            --matches-sig {input.matches} \
            --json-out {output.json} \
            --match-rank {params.match_rank}
    """

# compare taxonomy for contigs in a genome
rule compare_taxonomy_single:
    input:
        all_json = output_dir + '/{g}.contigs-tax.json',
        all_sig = output_dir + '/{g}.matches.sig',
        lineages = config['lineages_csv'],
        provided_lineages = provided_lineages_file,
        genome_list = genome_list_file
    output:
        hit_list = output_dir + '/{g}.hit_list_for_filtering.csv',
        contig_details = output_dir + '/{g}.genome_summary.csv',
        contam_json = output_dir + '/{g}.contam_summary.json',
    conda: 'conf/env-sourmash.yml'
    params:
        output_dir = output_dir,
        min_f_major = min_f_major,
        min_f_ident = min_f_ident,
        match_rank = default_match_rank,
    shell: """
        python -m charcoal.compare_taxonomy \
            --input-directory {params.output_dir} \
            --genome-list-file {input.genome_list} \
            --lineages-csv {input.lineages} \
            --provided-lineages {input.provided_lineages} \
            --hit-list {output.hit_list} \
            --contig-details-summary {output.contig_details} \
            --contam-summary-json {output.contam_json} \
            --min_f_ident={params.min_f_ident} \
            --min_f_major={params.min_f_major} \
            --match-rank={params.match_rank} \
            {wildcards.g}
    """

rule combine_hit_list:
    input:
        expand(output_dir + '/{g}.hit_list_for_filtering.csv', g=genome_list),
    output:
        output_dir + '/hit_list_for_filtering.csv'
    params:
        sort_by = f"{default_match_rank}_bad_bp"
    shell: """
        python -m charcoal.combine_csvs --sort-by {params.sort_by} --reverse \
            {input} > {output}
    """

rule combine_genome_summary:
    input:
        expand(output_dir + '/{g}.genome_summary.csv', g=genome_list),
    output:
        output_dir + '/genome_summary.csv'
    shell: """
        python -m charcoal.combine_csvs --sort-by genome {input} > {output}
    """

# generate contigs genbank matches given hitlist match_rank
rule make_contigs_matches_list:
    input:
        genome = genome_dir + '/{f}',
        genome_sig = output_dir + '/{f}.sig',
        matches = output_dir + '/{f}.matches.sig',
    output:
        accfile = output_dir + '/{f}.matches.acc.txt'
    conda: 'conf/env-sourmash.yml'
    params:
        match_rank = default_match_rank,
    shell: """
        python -m charcoal.contigs_search_matches \
            --genome {input.genome} \
            --genome-sig {input.genome_sig} \
            --matches-sig {input.matches} \
            --output {output.accfile} \
            --match-rank {params.match_rank}
    """

# combine acc list
rule combine_hitlist_accs:
    input:
        genome_accs = expand(output_dir + '/{f}.matches.acc.txt',
                             f=hitlist_genomes),
    output:
        accfile = output_dir + '/hitlist.acc.txt'
    shell: """
        sort {input.genome_accs} | uniq > {output.accfile}
    """

checkpoint split_hitlist_accs:
    input:
        accfile = output_dir + '/hitlist.acc.txt'
    output:
        accfiles = directory(output_dir + '/genbank/')
    run:
        try:
            os.mkdir(output_dir + '/genbank')
        except FileExistsError:
            pass
        for acc in open(input.accfile, 'rt'):
            acc = acc.strip()
            filepath = output_dir + f'/genbank/{acc}.txt'
            if acc and not os.path.exists(filepath):
                print(f'writing {acc} to {filepath}')
                with open(filepath, 'wt') as fp:
                    fp.write(acc)


def aggregate_input(wildcards):
    checkpoint_output = checkpoints.split_hitlist_accs.get(**wildcards).output[0]
    x = expand(output_dir + f"/genbank/{wildcards.acc}.txt",
           i=glob_wildcards(os.path.join(checkpoint_output, "{i}.txt")).i)
    return x

# download matching genbank genome details
rule make_genbank_matches_csv:
    input:
        aggregate_input
    output:
        csvfile = output_dir + '/genbank_info/{acc}.info.csv'
    conda: 'conf/env-sourmash.yml'
    shell: """
        python -m charcoal.genbank_genomes \
            {input} \
            --output {output.csvfile}
    """

# hitlist-dependent rule for downloading actual genomes
rule download_matching_genomes_one_by_one:
     input:
         csvfile = output_dir + '/genbank_info/{acc}.info.csv'
     output:
         genome = "genbank_genomes/{acc}.fna.gz"
     run:
         with open(input.csvfile, 'rt') as infp:
             r = csv.DictReader(infp)
             for row in r:
                 acc = row['acc']
                 if not wildcards.acc.startswith(acc): continue
                 url = row['genome_url']
                 name = row['ncbi_tax_name']

                 print(f"downloading genome for acc {acc}/{name} from NCBI...")
                 with open(output.genome, 'wb') as outfp:
                     with urllib.request.urlopen(url) as response:
                         content = response.read()
                         outfp.write(content)
                         print(f"...wrote {len(content)} bytes to {output.genome}")

# generate contigs taxonomy
rule post_hitlist_make_contigs_matches:
    input:
        genome = genome_dir + '/{f}',
        genome_sig = output_dir + '/{f}.sig',
        matches = output_dir + '/{f}.matches.sig',
        lineages = ancient(config['lineages_csv']),
        hitlist = ancient(output_dir + '/hit_list_for_filtering.csv')
    output:
        matches_yaml = output_dir + '/{f}.hitlist.matches.yaml',
    conda: 'conf/env-sourmash.yml'
    params:
        match_rank = default_match_rank,
    shell: """
        python -m charcoal.contigs_list_contaminants \
            --genome {input.genome} --lineages-csv {input.lineages} \
            --genome-sig {input.genome_sig} \
            --matches-sig {input.matches} \
            --hitlist {input.hitlist} \
            --yaml-out {output.matches_yaml} \
            --match-rank {params.match_rank}
    """

rule all_hitlist_matches:
    input:
        [ output_dir + f'/{f}.hitlist.matches.yaml' for f in hitlist_genomes ]

rule clean_contigs:
    input:
        genome = genome_dir + '/{f}',
        json = output_dir + '/{f}.contigs-tax.json',
        hit_list = output_dir + '/hit_list_for_filtering.csv',
    output:
        clean = output_dir + '/{f}.clean.fa.gz',
        dirty = output_dir + '/{f}.dirty.fa.gz',
    conda: 'conf/env-sourmash.yml'
    shell: """
        python -m charcoal.clean_genome \
            --genome {input.genome} \
            --hit-list {input.hit_list} \
            --contigs-json {input.json} \
            --clean {output.clean} --dirty {output.dirty}
    """

rule mash_compare:
    input:
        query = genome_dir + '/{f}',
        target = ancient('genbank_genomes/{acc}.fna.gz'),
    output:
        outfile = output_dir + '/{f}.x.{acc}.mashmap.out'
    conda: 'conf/env-mashmap.yml'
    shell: """
        mashmap -q {input.query} -r {input.target} -o {output.outfile} --pi 95
    """

rule do_hitlist_compare:
    input:
        [ output_dir + f'/{f}.x.{acc}.mashmap.out' for f, acc in hitlist_pairs ]

rule set_kernel:
    output:
        touch(f"{output_dir}/.kernel.set")
    conda: 'conf/env-reporting.yml'
    shell: """
        python -m ipykernel install --user --name charcoal
        python -m pip install pyinterval
    """

rule make_notebook_report:
    input:
        nb='charcoal/notebooks/report-genome.ipynb',
        summary=f'{output_dir}/genome_summary.csv',
        hitlist=f'{output_dir}/hit_list_for_filtering.csv',
        kernel_set = rules.set_kernel.output
    output:
        report_dir + '/{g}.fig.ipynb'
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.nb} - -k charcoal --cwd {report_dir} \
              -p directory .. -p render '' \
              -p name {wildcards.g:q} \
              > {output}
    """

rule make_html_report:
    input:
        notebook=report_dir + '/{g}.fig.ipynb',
        summary=f'{output_dir}/genome_summary.csv',
        hitlist=f'{output_dir}/hit_list_for_filtering.csv',
        contigs_json=f'{output_dir}/{{g}}.contigs-tax.json',
    output:
        report_dir + '/{g}.fig.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        python -m nbconvert {input.notebook} --to html --stdout --no-input --ExecutePreprocessor.kernel_name=charcoal > {output}
    """
     

rule make_notebook_alignment:
    input:
        nb='charcoal/notebooks/report-alignment.ipynb',
        summary=f'{output_dir}/{{g}}.hitlist.matches.yaml',
        kernel_set = rules.set_kernel.output
    output:
        report_dir + '/{g}.align.ipynb'
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.nb} - -k charcoal --cwd {report_dir} \
              -p directory .. -p genome_dir ../../{genome_dir:q} -p render '' \
              -p name {wildcards.g:q} \
              > {output}
    """

rule make_html_alignment:
    input:
        notebook=report_dir + '/{g}.align.ipynb',
        summary=f'{output_dir}/{{g}}.hitlist.matches.yaml',
    output:
        report_dir + '/{g}.align.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        python -m nbconvert {input.notebook} --to html --stdout --no-input --ExecutePreprocessor.kernel_name=charcoal > {output}
    """
     
rule all_alignment_reports:
    input:
        [ report_dir + f'/{f}.align.html' for f in hitlist_genomes ]

rule make_index:
    input:
        notebook='charcoal/notebooks/report-index.ipynb',
        summary=f'{output_dir}/genome_summary.csv',
        kernel_set = rules.set_kernel.output
    output:
        nb=f'{report_dir}/index.ipynb',
        html=f'{report_dir}/index.html',
    conda: 'conf/env-reporting.yml'
    shell: """
        papermill {input.notebook} - -p name {output_dir:q} -p render '' \
            -p directory .. -k charcoal --cwd {report_dir} > {output.nb}
        python -m nbconvert {output.nb} --to html --stdout --no-input > {output.html}
    """
